# 信息抓取中如何解决网站拒绝问题

最近，给二手车收集器(http://sche.mobi)增加了些功能，让它自维护，涉及到信息抓取。一下是其中一些问题的解决办法。

在信息抓取过程中，经常遇到的问题

1. 你被网站认为是程序访问，而不是个人通过浏览器访问

2.你访问的次数太频繁，而被拒绝访问

3.你同时建立的连接已经超过了通常的数量，而被拒绝访问。

针对这个问题，yaotian给出的解决办法核心思想如下：

1. 你建立的网络连接一定要模拟出浏览器的特征属性

2. 在访问同一个网站的时候，应该用连接池。连接池的意思是在本地列表中维护你已经创建出来的连接。当有连接需要的时候，从这个池中任意挑选一个。这样可以减少打开连接的次数和打开的数量

3. 遇到timeout的时候可以尝试多次。

一下给出例子

如何给出浏览器伪装


```
headers = {

    ‘User-Agent’:'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6′
}
req = urllib2.Request(
    url = ‘http://sche.mobi’,
    data = postdata,
    headers = headers
)
```

如何建立连接池和retry

	def retrieve_url(url,retries= 3):
	    url_parse = urlparse.urlsplit(url)
	    hostname = url_parse[ 1]
	    if hostname in http_connection_pool:
	        connections = http_connection_pool[hostname]
	        if len(connections) < 5:
	            opener = urllib2.build_opener(support, urllib2.HTTPHandler )
	            connections.append(opener)
	            http_connection_pool[hostname]=connections
	        else:
	            i = random.randint( 0, 4)
	            opener = connections[i]
	    else:
	        opener = urllib2.build_opener( support, urllib2.HTTPHandler )
	        http_connection_pool[hostname]=[opener]
	    try:
	        req = urllib2.Request(
	            url = url,
	            headers = headers
	        )
	        return opener.open(req).read()
	    except Exception:
	        if retries> 0:
	                return retrieve_url(url,retries- 1)
	        else:
	            logger.exception( “Can’t retrieve content from url:”+url)
	            return None


 在我的程序中出现任何问题，都会给我邮箱中发出错详细信息。以前每天都有抓取失败的邮件，采用了以上的方法后，基本上没有抓取失败的邮件了，太好了，喝咖啡去。